#!/usr/bin/env groovy

@Library("jenkins-devops-scripts") _
node('slave') {
  def gitUtils = new com.beat.utilities.github()
  def stackUtils = new com.beat.utilities.stack()
  def helmUtils = new com.beat.utilities.helm3(this)
  def terraformUtils = new com.beat.utilities.terraform()
  def jobName = gitUtils.determineRepoName(env.JOB_NAME)
  def project = "cx-invoicing-portal"
  def committer = ""
  def stagingBranch = "develop"
  def namespace = "customer-experience"
  def chartmuseumAPI = "https://chartmuseum.private.k8s.management.thebeat.co/api/charts"
  def slackChannel = "#cx-builds"
  def slackToken = "XOpkUSxts9Pjq79Jf4SRVU1b"
  def monitoringUtils = new com.beat.utilities.monitoring()
  def notificationUtils = new com.beat.utilities.notifications(slackToken, project, env.BUILD_URL, env.BUILD_NUMBER)
  notificationUtils.defaultServerDeploymentsChannel = slackChannel

  def helmName = "cx-invoicing-portal"

  // Define the stacks you want to deploy to
  def supportedStacks = ["mx", "qamx"]

  // Define whether we are deploying to production environments
  def isProduction = false

  stage('Clone repository') {
    try {
            echo "Clearing folder..."
            /* We delete the existing workspace for consistency. If we don't, git deleted files will still exist here. */
            deleteDir()

            if (env.TAG_NAME != null) {
              checkout scm
              echo "This is the ${env.TAG_NAME} tag"
              helmTag = env.TAG_NAME
              result = stackUtils.findStacksFromTags(env.TAG_NAME)
              stacks = result[0]
              isProduction = result[1]
              version = env.TAG_NAME
            } else {
              echo "This is the ${env.BRANCH_NAME} branch"

              if (env.BRANCH_NAME != stagingBranch) {
                  sh "exit 0"
              }
              // Clone repository to our workspace - including tags
              // the tags presence allows to calculate a nice version identifier when the HEAD commit is not tagged
              checkout([
                  $class: 'GitSCM',
                  branches: [[name: 'refs/heads/' + env.BRANCH_NAME]],
                  extensions: [[$class: 'CloneOption', noTags: false, shallow: false, depth: 0, reference: '']],
                  userRemoteConfigs: scm.userRemoteConfigs,
              ])
              stacks = stackUtils.stacksStaging()
              // use a descriptive version with commits and offset (when not matching a tag)
              version = gitUtils.describeWithTags()
              // This here is a hack/workaround!
              // helmTag is actually the helm chart's version. However helm does not support
              // semver incompatible versions, that is the version does not start with something
              // like vx.x.x or plain x.x.x
              // In our case, when we build for develop branch we have no version, since there is not git tag here.
              // So have a placeholder version of v1.0.0-develop
              helmTag = "v1.0.0-${env.BRANCH_NAME}"

            }

            // get committer name after checkout
            committer = gitUtils.determineGitCommiter(false)

            // Intersection of stacks
            stacks = stacks.intersect(supportedStacks)

            notificationUtils.slackNotifyDeploymentStarted(stacks)

            // Get Management stack variables
            envVarMapManagement = stackUtils.managementstackVariables()
            kubeConfigIDMgmt = "KUBECONFIG_${envVarMapManagement.KUBERNETES_CLUSTER}"
      } catch(e) {
            println("Exception on cloning repository")
            println(e.toString())
            def message = "deployment failed on `${STAGE_NAME}` at ${stacks} for ${project} <${env.BUILD_URL}console|${env.BUILD_NUMBER}>"
            notificationUtils.slackNotify('FAILURE', slackChannel, slackToken, message)
            throw e
        }
  }

  stage('Build docker image') {
    /* This builds the actual image; synonymous to docker build on the command line */
    img = docker.build("beat/${project}:${version}", "--build-arg version=${version} -f infra/deploy/local/Dockerfile .")
  }

  stage('Push image to registry') {
      // If image is built, push it to registry
      docker.withRegistry("https://${envVarMapManagement.REGISTRY_SERVER}") {
        img.push(version)
      }
  }

  stage("Helm lint") {
    try {
      helmUtils.helmLint(kubeConfigIDMgmt, "infra/deploy/helm/${project}")
    } catch (e) {
      println("Exception on Helm linting")
      println(e.toString())
      def message = "deployment failed on `${STAGE_NAME}` at ${stacks} for ${project} <${env.BUILD_URL}console|${env.BUILD_NUMBER}>"
      notificationUtils.slackNotify('FAILURE', slackChannel, slackToken, message)
      throw e
    }
  }

  stage("Helm Build Package") {
     helmUtils.helmPackage(kubeConfigIDMgmt, "infra/deploy/helm/${project}", helmTag)
  }

  stage("Push helm package to chartmuseum") {
     helmUtils.helmPush(project, helmTag, chartmuseumAPI)
  }

  for (stack in stacks) {

    stage("Deploy helm chart") {
      def setList = [:]

      if (isProduction) {
        envVarMapping = stackUtils.stackVariables(stack)
      }
      else {
        envVarMapping = stackUtils.stackVariables('dev')
      }

      kubeConfigID = "KUBECONFIG_${envVarMapping.KUBERNETES_CLUSTER}"

      try {
        def startTime = new Date().getTime()
        withCredentials([
            string(credentialsId: 'GRAFANA_MANAGEMENT_GENERIC_PIPELINE_API_KEY', variable: 'grafanaAPIKey'),
        ]) {

          setList += [
            'image.tag': version,
            'changeCause': "Jenkins=${RUN_DISPLAY_URL} Commiter=${committer}",
            'env.STACK' : stack
          ]

          if (isProduction) {
            setList += [
              'env.REACT_APP_API': "https://cx-invoicing.k8s.${envVarMapping.KUBERNETES_CLUSTER}/api",
              'ingress.clusterSuffix': "k8s.${envVarMapping.KUBERNETES_CLUSTER}",
              'ingress.className': 'public',
              'env.REACT_APP_ONE_LOGIN_AUTHORITY': 'https://taxibeat.onelogin.com/oidc/2',
              'env.REACT_APP_ONE_LOGIN_CLIENT_ID': '064352d0-d534-013a-002d-02bb6e2eb1d774788',
              'env.REACT_APP_ONE_LOGIN_REDIRECT_URI': "https://${project}.k8s.${envVarMapping.KUBERNETES_CLUSTER}/auth",
            ]
          } else {
            // do not enable autoscaling on staging (note that if we do, deployment will fail)
            setList += [
              'env.REACT_APP_API': "https://${stack}-cx-invoicing.private.k8s.${envVarMapping.KUBERNETES_CLUSTER}/api",
              'ingress.clusterSuffix': "private.k8s.${envVarMapping.KUBERNETES_CLUSTER}",
              'ingress.className': 'private',
              'env.REACT_APP_ONE_LOGIN_AUTHORITY': 'https://automations-dev.onelogin.com/oidc/2',
              'env.REACT_APP_ONE_LOGIN_CLIENT_ID': 'ce21d630-ca73-013a-9439-061f952d4a17210837',
              'env.REACT_APP_ONE_LOGIN_REDIRECT_URI': "https://${stack}-${project}.private.k8s.${envVarMapping.KUBERNETES_CLUSTER}/auth",
              'autoscaling.enabled': false,
              'ingress.host': "${stack}-${project}",
            ]

            helmName = "${project}-${stack}"
          }

          if (version != null) {
            setList += [
                'main.version': version,
            ]
          }

          setList = setList.collect { key, value -> "--set ${key}=\"${value}\"" }.join(" ")
          helmUtils.helmDeploy(kubeConfigID, helmTag, namespace, setList, helmName, "beat/${project}")
          def clusterTag = isProduction ? 'prod' : 'dev'
          def grafana = new com.beat.utilities.grafana("https://grafana.private.k8s.management.thebeat.co", grafanaAPIKey)
          grafana.createAnnotation(startTime, new Date().getTime(), (String[])[namespace, project, clusterTag, stack, version], "Deployment")
          currentBuild.result = 'SUCCESS'
        }
      } catch (ex) {
        echo "Exception (${ex.toString()}) occurred with message: ${ex.getMessage()}"
        echo "Start rolling back ${kubeConfigID}, ${helmName}"
        helmUtils.helmRollback(kubeConfigID, 0, helmName, namespace)
        notificationUtils.slackNotifyDeploymentFailure(stack, STAGE_NAME)
        throw ex
      }

      notificationUtils.slackNotifyDeploymentSuccess(stack)
    }

    stage('Apply Prometheus rules if available') {
      def replacements
      if (isProduction) {
        replacements = [
          '__STACK__': stack,
          '__COUNTRY__': stackUtils.stackCountry(stack)
        ]
      } else {
        replacements = [
          '__STACK__': 'dev',
          '__COUNTRY__': 'sandbox'
        ]
      }
      try {
        sh "git checkout infra/observe/alerting"
        monitoringUtils.prometheusConfig(kubeConfigID, namespace, "infra/observe/alerting", replacements)
      } catch (e) {
        println("Exception on applying prometheus rules")
        println(e.toString())
        def message = "deployment failed on `${STAGE_NAME}` at ${stack} <${env.BUILD_URL}console|${env.BUILD_NUMBER}>"
        notificationUtils.slackNotify('FAILURE', slackChannel, slackToken, message)
        throw e
      }
    }
  }

  stage('Apply Grafana rules if available') {
    try {
      monitoringUtils.grafanaManagementConfig(namespace, "infra/observe/dashboard")
    } catch (e) {
      println("Exception on applying grafa rules")
      println(e.toString())
      def message = "deployment failed on `${STAGE_NAME}` at ${stack} <${env.BUILD_URL}console|${env.BUILD_NUMBER}>"
      notificationUtils.slackNotify('FAILURE', slackChannel, slackToken, message)
      throw e
    }
  }
}
